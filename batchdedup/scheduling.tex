\section{Round Scheduling}
\label{sect:scheduling}
The simplest way to take advantage of the efficiency of batch processing is to
schedule all the work to be done in one round.  This works well if all of the
data being backed up are only copies, e.g. a separate backup system which data
is sent to over the network. However, in our case we are backing up the
original virtual disk, which may still be in use during the snapshot. This adds
extra complexity to the cost analysis, because we must now also consider the
cost of maintaining a constistent view during the snapshot process. We use the
Copy on Write (CoW) provided by the virtual disk manager to maintain a
consistent view. With CoW, the
duration of the backup affects how much data must be copied. Other studies have
shown that as much as 8\% or even more of total capacity must be reserved for
CoW \cite{EMCIncrementalDataChanges}. The actual cost of CoW is a factor of the
data size, the write rate, and duration CoW is taking place. The data size
isn't something we can change, nor the write rate, but we can minimize the
duration that a given VM is undergoing CoW. We assume a poisson distribution of
writes (which closely fits the meaured results from
\cite{EMCIncrementalDataChanges}), and then try to minimize the CoW cost using
our performance and CoW model. Although the single batch schedule completes the
backup in the smallest amount of time, it also has the greatest CoW cost
because the most processing must be done before any VM can release the CoW
lock.

Our basic CoW cost model is:
\[
    CoW=n(1-e^{-m/n})
\]
    where
\[
    m=tw(1-d_1)c
\]

In the above model, $n$ is the number of dirty segments, $t$ is the time under
CoW, $w$ is the
write rate during CoW, $d_1$ is the \% of clean data (which doesn't go under
CoW), and $c$ is a constant determining how likely writes are to touch dirty
segments which haven't yet been copied vs. dirty segments which have already
been copied during the current backup. With this CoW cost model and the earlier
performance model, we can estimate the CoW cost of a given backup schedule.
Note that CoW ends in Stage 2b, so only the time up to the end of 2b counts
towards CoW. \todo{we still need to pick a value for $c$}

We can see above that the way to decrease the CoW cost is to decrease t, the
time the virtual disk is under CoW. We do that by breakng up the backup into
multiple
rounds, where in each round the CoW cost is minimized. The more rounds there
are the shorter each round can be and therefore the smaller the CoW cost. The
more rounds there are however the greater the backup overheads and some of the
efficiency gained from batch processing is lessened. We balance these costs by
setting a time limit on the whole backup job, and then develop an algorithm to
schedule VMs into rounds and pick the number of rounds to use.

With these parameters a model that closely fits our goals is the dual version
of the
bin packing problem. In standard bin packing the goal is to fit all of the
items into as few bins as possible, without overfilling any bins. In the dual
version of the problem however as many bins as possible are to filled to at
least some minimum level. This fits our purpose because we want to maximize the
number of rounds, with some constraint to keep the efficiency benefits of batch
processing. In our dual-bin-packing solution the constraint is to keep the total
cost of the schedule under a time limit rather than mandating a minimum bin
level. We
adapt an algorithm for dual bin packing\cite{DualBinPacking} to fit our VM
scheduling problem. The algorithm adapted is called iterated A and works by
iteratively callng a bin packing heuristic A with the items to be scheduled and
the number of bins, using binary search to arrive at the best number of
bins. A(I,N) is defined to return the optimality of packing set I into N bins
using A. We take this basic idea and look at several VM packing heuristics to
arrive at an efficient packing algorithm. More formally, our adaptation of the
iterated A algorithm is defined as:


%lstset{basicstyle=\ttfamily\tiny
%}
\todo{justify choice of initial UB (right now it is mostly arbitrary)}
\begin{lstlisting}
Set UB=min(n,2*v)
Set LB=1
while UB>LB
  set N = (UB+LB+1)/2
  if A(machines,N) > T, set UB=N-1
  else set LB=N
Halt
\end{lstlisting}
where A(machines,N) returns the total backup time of the schedule for N rounds\\
This algorithm returns the packing generated by A(machines,UB) after loop finishes

The general algorithm relies on a good choice of A to arrive at an efficent
packing. Our first VM packing heuristic, A0, is a na\"\i{}ve approach very
close to the un-adapted algorithm from the dual bin packing paper. For both
algorithms we develop, in case of ties, choose the left-most item.

A0:
\begin{lstlisting}
sort VMs in descending order by size
while there is an unscheduled VM
  pick the first unscheduled VM a
    pick the round with the current
    lowest runtime b
  schedule VM a to round b
Halt
\end{lstlisting}

A0 decreases CoW cost (see Table~\ref{tab:schedule-costs}), but has room for
improvement. The first issue is that the round with the current lowest runtime
is chosen.  Because backup time is dependent on a combination of the highest
machine load and average machine load, if we add a VM to the currently most
loaded machine in a round it will increase runtime much more than if we add the
VM to a different round where that machine is currently empty
\todo{(insert figure to show how this happens)}. Therefore a better scheduling
would be obtained if we pick the round with the lowest runtime after we
simulate adding the new VM to that round. This new round picking heuristic
takes into account that the same VM might have different affects on different
rounds. This aspect of round picking must be considered because we assume a VM
must be backed
up by the machine that hosts it. If the VMs are located on a DFS such that they
can be
backed up by any machine in the cluster then the problem of load balancing
becomes a simpler but much different problem, and isn't considered here. We
also pick the most heavily loaded (i.e. most data remaing) machine in case of
ties so we can make the most backup progress in a round.

Another improvement we can make to A0 is to the VM picking heuristic. For the
same reason as given above, the largest VM may not always have the greatest
impact on time (e.g. picking a slightly smaller VM on the most heavily loaded
machine has a greater impact than selecting the largest VM on a lightly loaded
machine). A better way to pick VMs than just by size is to simulate removing
VM's, then model the new single round time, and pick the VM whose removal most
decreases the single round time (i.e. the VM that has the greatest impact on
overall backup time). By picking VMs this way we take into account
the machine load in picking which VM to schedule next, and minimize the time
the remaining VMs will take to backup

After making the two above changes we arrive at VM packing heuristic A1.

A1:
\begin{lstlisting}
while there is an unscheduled VM
  pick the VM a whose removal will
    most decrease single round time
    tie-breaker:VM on most heavily
    loaded round
  pick the round b whose runtime will
    be lowest after adding VM a
  schedule VM a to round b
Halt
\end{lstlisting}

A1 significantly improves our simulated results, bringing our CoW costs much
closer to the best case than the worst case, as can be seen in
Figure~\ref{tab:schedule-costs}.  Our current implementation of the algorithm
focuses on the deduplication and so doesn't make use of CoW, but we can see
that our simulated times closely match measured runtimes (hopefully). \todo{I
haven't actually implemented the schedulers into the dedup yet to test this}
