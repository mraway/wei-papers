\comments{
\section{Design Overview}
\label{sect:design}
%[describe what is going to be presented in this section]
We start by analyzing the main features in the design of our system.
We first present its architecture,
then introduce the two major contributions: the collocated deduplication 
scheme and the snapshot deletion strategy.

\begin{figure}
    \centering
    \subfigure[Cluster architecture]
    {
        \includegraphics[width=3in]{images/socc_arch_cluster}
        \label{fig:arch_cluster}
    }
    \\
    \subfigure[Node architecture from VM point of view]
    {
        \includegraphics[width=3in]{images/socc_arch_vm}
        \label{fig:arch_vm}
    }
    \caption{System architecture}
    \label{fig:arch}
\end{figure}

\subsection{Architecture}
\label{sect:arch}
%[describe the cloud environment]
Our architecture, as shown in figure.\ref{fig:arch}, is built on top of 
Alibaba's platform which is the largest cloud service provider in China. 

%[describe the arch from cluster side]
{\bf Cluster Architecture}
A typical VM cluster in our cloud environment
consists of from hundreds to thousands of physical machines, each of which can
host tens of Xen-based\cite{Barham2003} VMs.
Alibaba's cloud platform provides a hadoop-like infrastructure, 
which includes several highly scalable distributed service:
\begin{enumerate}
\item {\bf Distributed file system} This is a scalable distributed file system (DFS) being optimized for many large and sequential reads or appends. DFS holds the responsibility of managing physical disk storage
in the cloud. All data needed for VM services, such as virtual disk images used by runtime VMs,
and snapshot data for backup purposes, reside in this distributed file system. 
%\item[KV]: a distributed key-value store for managing structured data.
%\item[MapReduce]: a distributed data processing framework supports Map-Reduce\cite{Dean2004}.
\item {\bf Distributed memory cache} A distributed memory object caching system helps us to hold the fingerprints of those popular data blocks for deduplication inquiries. 
\end{enumerate}
}

\section{Architecture and Implementation Details}
\label{sect:arch}
%[describe what is going to be presented in this section]
%We start by analyzing the main features in the design of our system.
%We first present its architecture,
%then introduce the two major contributions: the colocated deduplication 
%scheme and the snapshot deletion strategy.
%\subsection{Architecture}
%[describe the cloud environment]


%Our architecture, as shown in figure.\ref{fig:arch}, is built on top of 
%Alibaba's platform which is the largest cloud service provider in China. 
%A typical VM cluster in our cloud environment
%consists of from hundreds to thousands of physical machines, each of which can
%host tens of xen-based\cite{Barham2003} VMs.
%Alibaba's cloud platform provides a hadoop-like infrastructure, 
%which includes several highly scalable distributed service:

Our system runs on a cluster of Linux machines with Xen-based VMs.
A distributed file system (DFS) manages  the physical disk storage and we use 
QFS~\cite{QFS}. 
All data needed for VM services, such as virtual disk images used by runtime VMs,
and snapshot data for backup purposes, reside in this distributed file system. 
One physical node hosts tens of VMs, each of which access its virtual machine disk image through the
virtual block device driver (called TapDisk\cite{Warfield2005} in Xen).

\comments{
\begin{description}
\item[Distributed file system] This is a scalable distributed file system (DFS) being optimized for many large and sequential reads or appends. DFS holds the responsibility of managing physical disk storage
in the cloud. 
All data needed for VM services, such as virtual disk images used by runtime VMs,
and snapshot data for backup purposes, reside in this distributed file system. 
%\item[KV]: a distributed key-value store for managing structured data.
%\item[MapReduce]: a distributed data processing framework supports Map-Reduce\cite{Dean2004}.
\item[Distributed memory cache]: A distributed memory object caching system helps us to hold the fingerprints of those popular data blocks for deduplication inquiries. 
\end{description}

In addition, our implementation also uses MapReduce to facilitate the offline
data processing, and stores a small amount of snapshot metadata in a 
BigTable-like persistent key-value store. 

%[describe the benefits of using mature cloud technologies]
In general, our snapshot system and the virtual machine management service 
rely on these fundamental cloud services
to be functional. Such a co-located deduplication architecture allow us 
to enjoy the benefits from these mature technologies 
such as load balancing, scalability, and fault tolerance.
Moreover, all the above cloud services can easily find their open-source counterparts,
which improves the generality of our architecture and deduplication scheme.


%[describe the architecture frm node side]
{\bf Node Architecture} 
Our node-side architecture, depicted in figure.\ref{fig:arch_vm}, consists of
two main entities: a virtual block device driver, and snapshot deduplication component.

%[brief the virtual device driver]
In Alibaba's VM cloud, every VM access its virtual machine disk image through the
virtual block device driver (called TapDisk\cite{Warfield2005} in Xen).
This driver maintains a map of dirty-bits to record
the change status of every fix-size segment of the virtual disk. 
When the VM issue a disk write, the bits corresponding to the segments that covers 
the modified disk region are set, thus letting snapshot deduplication component knows these
segments must be checked during snapshot backup. After the snapshot backup is finished, 
snapshot deduplication component acknowledges the driver to resume the dirty-bits map to
a clean state.

%[brief the snapshot deduplication]
The snapshot deduplication component consists of the chunking and deduplication 
logic of our snapshot storage system. We choose 

It contains an append store client which provides facilities to manage stored snapshot data, and a PDS client to support PDS index access. We will further discuss our deduplication scheme in section\ref{sect:dedupe}.


{\bf Snapshot Representation}
The virtual device driver uses a bitmap to track the changes 
that have been made to virtual disk.
Every bit in the bitmap represents a fix-sized (2MB) region called \textit{segment}, indicates whether the segment
is modified since last backup. Hence we could treat segment as the basic unit 
in snapshot backup similar to
file in normal backup: a snapshot could share a segment with previous snapshot it is not changed. 
Moreover, we break segments into var-sized chunks (average 4KB) using content-based chunking algorithm,
which brings the opportunity of fine-grained deduplication by
allowing data sharing between segments.
}

\subsection{ Components of a cluster node } 

As  depicted in Figure~\ref{fig:arch_vm}, 
there are four key service components running on each cluster
node  for supporting backup and deduplication: 
1) a virtual block device driver, 2) a snapshot deduplication component,
3) an append store client to store  and access snapshot data,
and 4)  a PDS client to support PDS index access. We will further discuss our deduplication scheme 
in Section~\ref{sect:dedupe}.


We use the virtual device driver in Xen that employs a bitmap to track the changes 
that have been made to virtual disk.
%This driver maintains a map of dirty bits to record the change status of every fix-size segment of the virtual disk. 
When the VM issue a disk write, the bits corresponding to the segments that covers 
the modified disk region are set, thus letting snapshot deduplication component knows these
segments must be checked during snapshot backup. After the snapshot backup is finished, 
snapshot deduplication component acknowledges the driver to resume the dirty-bits map to
a clean state.
Every bit in the bitmap represents a fix-sized (2MB) region called \textit{segment}, indicates whether the segment
is modified since last backup. Hence we could treat segment as the basic unit 
in snapshot backup similar to
file in normal backup: a snapshot could share a segment with previous snapshot it is not changed. 
As a standard practice, segments are further divided into variable-sized chunks (average 4KB) 
using content-based chunking algorithm, which brings the opportunity of fine-grained deduplication by
allowing data sharing between segments.

The representation of each snapshot has  a two-level index data structure.
%in the form of a hierarchical directed acyclic graph as shown in Figure \ref{fig:snapshot}.
The snapshot meta data (called snapshot recipe) contains a list of segments, each of which contains segment
metadata of its chunks (called segment recipe).
%\begin{figure}[htbp]
%  \centering
%  \epsfig{file=images/snapshot_representation, width=3in}
%  \caption{An example of snapshot representation.}
%  \label{fig:snapshot_rep}
%\end{figure}
%As a result, the representation of each snapshot is designed as a two-level index data structure 
%in the form of a hierarchical directed acyclic graph as shown in Figure \ref{fig:snapshot_rep}.
%A snapshot recipe contains a list of segments, each of which is represented as a segment recipe
%that holds the meatdata of its chunks. We choose this two-level structure because in practice we
%observe that during each backup period only a small amount of VM data are added or modified. 
%As the result, even the metadata of two snapshots can be highly similar, 
%thus aggregating a large number of chunks as one segment can significantly reduce the space cost of snapshot metadata.
%Furthermore, instead of using variables-sized segments, we use a dirty bit to capture the change status of fix-sized
%segments which greatly ease the segment-level deduplication.
In a snapshot recipes or a segment recipe, 
the data structures  includes reference pointers to the actual data location.
%In our implementation the data reference is a 8 bytes field which is either an 
%ASID (discuss in Section \ref{sect:append}) or an offset of an additional flag indicates
%the location of PDS data.





%\subsection{Snapshot management}


\begin{figure*}[t]
    \centering
    \includegraphics[width=6in]{images/socc_arch_cluster}
    \caption{System Architecture}
    \label{fig:arch_vm}
\end{figure*}

\comments{%socc_arch_cluster now includes the entire arch instead of having 2 figures
\begin{figure}
    \centering
    \subfigure[Node architecture from VM point of view]
    {
        \includegraphics[width=3in]{images/socc_arch_vm}
        \label{fig:arch_vm}
    }
    \\
    \subfigure[Cluster architecture]
    {
        \includegraphics[width=3in]{images/socc_arch_cluster}
        \label{fig:arch_cluster}
    }
    \caption{System architecture.}
    \label{fig:arch}
\end{figure}
}

%[describe the architecture frm node side]
%[brief the virtual device driver]
%One physical node hosts tens of VMs, each of which access its virtual machine disk image through the
%virtual block device driver (called TapDisk\cite{Warfield2005} in Xen).
%This driver maintains a map of dirty-bits to record
%the change status of every fix-size segment of the virtual disk. 
%When the VM issue a disk write, the bits coresponding to the segments that covers 
%the modified disk region are set, thus letting snapshot deduplication component knows these
%segments must be checked during snapshot backup. After the snapshot backup is finished, 
%snapshot deduplication component acknowledges the driver to resume the dirty-bits map to
%a clean state.

%[brief the snapshot deduplication]
%The snapshot deduplication component consists of the chunking and deduplication 
%logic of our snapshot storage system. We choose 

%[describe the arch from cluster side]


%[describe the data structure in underlying storage]
\subsection{A VM-centric snapshot store for backup data}

We build a snapshot storage on the top of a distributed file system.
Following the VM-centric idea for the purpose of fault isolation,
each VM has its own snapshot store, containing new data chunks which are considered
to be non-duplicates.
There is also a special store containing all PDS chunks shared among different VMs.
The replication degree of file blocks of snapshot stores in the underlying file system
Extra replication of this store is added as discussed in Section~\ref{sect:analysis}.
As shown in Fig.\ref{fig:as_arch}, we explain the data structure of snapshot stores as follows.

\begin{itemize}
\item The PDS snapshot contains a set of commonly used data chunks and is accessed by its offset and size
in the corresponding DSF file.
The PDS index uses the offset and size as a reference in its index structure.
 
\item Each non-PDS snapshot store is divided into a set of containers and each of them is approximately 
1GB. The reason we divide the snapshot into containers is to simplify the compact process
conducted periodically.  There are chunks deleted without any reference from other snapshots
and to reclaim the space of these useless chunks, a compaction  routine can work on one container at a time
and copy used data chunks to another container.
	\begin{itemize}
	\item Each container is divided into a set of chunk data groups. Each group is composed of
	a set of data chunks and is the basic unit for the snapshot in data access and retrieval. 
	In writing a chunk group, data chunks in this group are compressed together and then stored. 
	When accessing a particular chunk, its chunk group is retrieved from the disk storage
	and uncompressed. Given the high spatial locality 
in snapshot data accessing~\cite{Sampling,FoundationPaper},
	retrieval of  data chunks by group naturally fits in the prefetching scheme  to speedup
	snapshot access. A  typical chunk group contains 100 to 1000 chunks, with an average size of 
200-600KB.
Chunk grouping also reduces the  container index size as we discuss below. 
Given  the average chunk size  4KB,  the index size for a 1GB container reduces from 10MB to 100KB when
the chunk group size is 100.
%%CBlock, then the overall index size is reduced to $1/m$. 
%n our implementation, using $m=100$ reduces the index for
%a 1GB container from 10 MB to 100 KB.
	\item 
Each data container is represented by three data files in the DFS:
1) the container data file holds the actual content of data chunks, 
2) the container index file is responsible for translating a data reference
into its location within a container, and 
3) a chunk deletion log file saving all the deletion requests within  the container.
A VM snapshot store typically has a small number of containers because each container is fairly large with an average size of 1GB, it 
maintains a limited number of snapshots (e.g. 10 in the Alibaba case),
and new snapshot data chunks can be effectively compressed in chunk groups in addition to  deduplication.

\item We maintain  a chunk counter and assign the current number 
as a chunk ID (called CID) within this container as a reference of a new chunk added to a container. 
Since data chunks are always appended to the snapshot store, 
 a CID is monotonically increasing.
A data chunk reference stored in the index of snapshot recipes
is composed of two parts: a container ID (2 bytes) and CID (6 bytes).
Once a snapshot is to be accessed access, the receipt in this snapshot will point to either a data chunk
in the PDS or a reference number with a container ID and CID.
With a container ID, the corresponding container index file is accessed and 
the chunk group is identified using this CID. Once this chunk group is loaded to memory, its header contains the exact offset of the corresponding chunk and the content is then accessed from the memory buffer.

%Every container  Store assign every piece of data a CID for its internal data referencing. 
%When new data is appended, its CID is the current largest CID in that container plus one.
%As a result, all the data locations are naturally indexed by this self-incremental CID, 
%no extra sorting is needed.
	\end{itemize}
\end{itemize}

%acceretrieved  and ret
%Using CBlock brings us several advantages: First, the write workload to DFS master is greatly reduced; second, grouping
%small chunks gives better compression. Third, reading a CBlock (200 - 600 KB) typically cost the same amount of disk 
%seek as reading a 4KB chunk. Finally, this greatly reduces the size of index. Let $m$ be the number of chunks in each
%CBlock, then the overall index size is reduced to $1/m$. In our implementation, using $m=100$ reduces the index for
%a 1GB container from 10 MB to 100 KB.

 
% of  divided into a set of data containers.
%Each container has 3 parts.
%Each chunk in the snapshot is referenced by the following data format

%The Append Store (AS) is our underlining storage engine for storing snapshot data in the distributed file system
%after deduplication.
%AS is built on top of our highly scalable distributed file system (DFS), 
%which is very similar to Google's file system
%in a sense that it is optimized for large files and sequential read/append operations.

\begin{figure}[htbp]
  \centering
  \epsfig{file=images/sstore_arch, width=3in}
  \caption{Data structure of VM snapshot stores.}
  \label{fig:as_arch}
\end{figure}

%\begin{figure}[htbp]
%  \centering
%  \epsfig{file=images/pds_arch, width=3in}
%  \caption{System architecture for PDS data store.}
%  \label{fig:as_arch}
%\end{figure}

The snapshot  store supports three API calls.
\begin{itemize}

%AS supplies three interfaces: {\em get(ref)} accepts a data reference and retrieves data, 
\item {\em Put(data)} places data chunk into the snapshot store and returns a reference to be stored in 
the recipe metadata of a snapshot. 

The write requests to append data chunks to a VM store are accumulated in the client side. 
When the number of write requests reaches the group size $g$, the snapshot store client compresses
the accumulated   chunk group, adds a chunk group index  to the beginning of the group, and then
appends the header and data  to the corresponding VM file.
A new container  index entry is also created for each chunk group and is written the corresponding
container index file.

The writing of PDS data chunks is conducted periodically when there is a new PDS calculation.
Since the PDS dataset is small, a new PDS file is created during the periodical update.
\item{\em Get(reference)}.
The fetch operation for the PDS data chunk is straightforward since each reference contains 
the offset and size within the PDS  underlying  file.
We also maintain a small data cache for the PDS data service to speedup the process.

To read a non-PDS chunk using a reference,  the snapshot store client first loads the
corresponding VM's container index file specified by the container ID, then searches the chunk
group  that  covers the chunk by the group  CID range.
After that, it reads the whole chunk group from DFS, decompresses it, seeks the exact chunk data 
specified by the CID. 
Finally, the client updates its internal chunk data cache with the newly loaded content to 
anticipate future sequential reads.
\item {\em Delete(reference)}.
A data chunk can be deleted when a snapshot expires or gets deleted explicitly by a user.
We will discuss the snapshot deletion shortly in the following subsection.
%deletes the data pointed by the reference.
%Under the hood, small var-sized data are grouped and stored into larger data containers. Each VM has
%its snapshot data stored in its own Append Store, specified by the VM ID. 
%We split every Append Store into multiple data containers so that reclaiming the disk space would not 
%result in rewriting all the data at the same time.
When deletion requests are issued for a specific container,
those requests are simply logged into the  container's deletion log initially and thus  a lazy
deletion strategy is exercised.
Once CIDs appear in
the deletion log, they will not be referenced by any future snapshot and can be safely deleted when needed. 
Periodically, the snapshot  store picks those containers with an excessive
number of deletion requests to  compact and  reclaim the corresponding disk space. 
%The actual compaction will only take place when the number of deleted items 
%reached $d\%$ of container's capacity. 
During compaction, the snapshot store creates a new container (with the same container ID) to replace the 
existing one. This is done by sequentially scan the old container, copying all the chunks that are not 
found in the deletion log to the new container, creating new chunk groups and indices. 
However, every chunk's CID is directly copied rather than re-generated. This
process leaves holes in the CID values, but does not affect the sorted order.
As a result, all data references stored 
in upper level recipes are permanent and stable, and the data reading process
is as efficient as before. This CID stability also ensures that recipes do not
depend directly on physical storage locations.
\end{itemize}

%As shown in Fig.\ref{fig:as_arch}, every data container is represented as three data files in DFS:
%the data file holds all the actual data, the index file is responsible for translating data reference
%into data locations, and a deletion log file remembers all the deletion requests to the container.
%
%A data reference is composed of two parts: a container ID (2 bytes) and CID (6 bytes).
%Append Store assign every piece of data a CID for its internal data referencing. 
%When new data is appended, its CID is the current largest CID in that container plus one.
%As a result, all the data locations are naturally indexed by this self-incremental CID, 
%no extra sorting is needed.

%Append Store groups multiple chunk data (i.e., 100) into larger units, called {\em CBlock}.
%CBlock is the basic unit for append store's internal read/write/compression.
%There is one index entry in the container index corresponding to every CBlock. It keeps the first chunk's CID
%in that CBlock, and the CBlock data's size and location.
%
%Using CBlock brings us several advantages: First, the write workload to DFS master is greatly reduced; second, grouping
%small chunks gives better compression. Third, reading a CBlock (200 - 600 KB) typically cost the same amount of disk 



\comments{
\subsection{Snapshot Deduplication and Fault Isolation}
\label{sect:dedupe}
%snapshot representation
%\subsection{Inner and corss VM deduplication}
Our  deduplication scheme compares the fingerprints of the current snapshot
with its parent snapshot and also other snapshots in the entire cloud.
This process performs   the duplication in two categories: \textit{Inner-VM} and \textit{Cross-VM}. 
Inner-VM duplication exists between VM's snapshots, because the majority of data are unchanged during each backup period. 
Such localization increases data independence between different VM backups,
simplifies snapshot management and statistics collection,
and facilitates parallel execution of snapshot operations.
On the other hand, Cross-VM duplication is mainly due to widely-used software and libraries. 
As the result, different VMs tend to backup large amount of highly similar data.
Our multi-level pipeline process can minimize 
the cost of deduplication while maximize the its efficiency at each level,
and it is parallel since each segment is processed independently.

\begin{itemize}
\item \textbf{Dirty-based coarse-grain inner-VM deduplication.}
The first-level deduplication is to follow the standard dirty bit approach, but is conducted
in the coarse grain segment level.
We use the  Xen virtual device driver which supports dirty bits for the storage device
and the dirty bit setting is maintained in a coarse grain level we call it a segment.
In our implementation, the segment size is 2MB. 
Since every write for a block will touch a dirty bit, the device driver maintains dirty bits in memory
and cannot afford a small segment  size.

\item \textbf{Chunk-level fine-grain nearby duplicate detection.}

The best deduplication uses a nonuniform chunk size 
in the average of 4K or 8K~\cite{??}.
Thus the second-level inner-VM deduplication is to assess in this
level, but only for those dirty  segments. 
We load the fingerprints of block chunks of the corresponding segment from the
parent for a comparisons and compare near-by fingerprint matching within this segment.
The amount of memory for maintaining those fingerprints  is small.
For example, with a 2MB segment, there are about 500 fingerprints to compare.


%If we use 4KB in level-1, then such a level-1 should have similar dedup efficiency 
%as the current level-1 and level-2 combined, because finally they equal to comparing  
%parent snapshot at 4KB granularity.
%
%However, at level-3 things are different. If we use 4KB fix-size block uniformly, it would be harder for different VM to share data through PDS, because there is no guarantee that the location of duplicate data on different VM disks are always aligned at 4KB boundary. For example, if two VMs each has a copy of duplicate data, but they are not aligned, then we won't be able to detect them. Our study at the current small data set has shown that using 4KB fix-size block will make PDS method less efficient by nearly 10%. Over the long time, more and more OS variations will co-exist in the cluster, making this 4KB fix-size approach inefficient in reducing duplicate data across VMs.

%\item \textit{Level 2  Chunk fingerprint comparison.}
%If a segment is modified, we perform fine-grained deduplication 
%by comparing the fingerprints of its chunks to the same segment's recipe in the previous snapshot,
%thus eliminate partial data duplication within the segment.
%\end{itemize}
%
%In general, operations at level 1 have almost no cost and most of unmodified data are filtered here. 
%To process a dirty segment at level 2, 
%there requires no more than one DFS access to load the segment recipe from previous snapshot,
%and a tiny amount of memory to hold it in main memory.

\item \textbf{Cross-VM deduplication.}
This step accomplishes the standard global fingerprint  comparison as conducted
in the previous work~\cite{??}.
One key observation is that the inner deduplication has removed many of duplicates.
There are not lot of deduplication opportunities cross VMs while the memory
consumption for global comparison is expensive.
Thus our approximation is that duplicate sharing patterns among  VM follows
a zip-like distribution, and the global fingerprint  comparison  only searches
for the most popular items. 
\end{itemize}

{\bf Popular Chunk Management and VM-oriented Fault Isolation}
Our objective for fault isolation is to minimize the number of VMs affected when there are failures
in the cluster.  The inner-VM deduplication does not depend on any global service and the comparison
for each VM is localized within the parent and the current snapshot.
Thus there is no data dependence between VMs.
For cross-VM deduplication, there is a data dependence among VMs and we would minimize the failure impact
of shared blocks by adding extra replicas of those shared blocks.

This section analyzes the choice of popular blocks and its impact on the deduplication efficiency.
It also  compares the  fault resilience of our VM-centric deduplication approach with a standard approach using 
global deduplication.


{\bf Impact of PDS deduplication.}
Our empirical study based on VM images from production environment\cite{ieeecloud} showed that the
frequency of data duplication follows Zipf-like distribution\cite{zipf},
with the exponent $\alpha$ between 0.65 ~ 0.70.
As a result, it can be proved that deduplication efficiency of PDS index is scalable:

For the Zipf-like distribution, an approximation to the sum of the first $n$ 
elements of the distribution can be derived as follows:
\begin{equation}
\sum_{i=1}^{n}\frac{1}{i^\alpha}\approx \int_{1}^{n}\frac{1}{x^\alpha}\mathrm{d}x=\frac{x^{1-\alpha}}{1-\alpha}=\frac{n^{1-\alpha}}{1-\alpha}\;  for\;  \alpha<1
\end{equation}
So the cumulative distribution function for a PDS holding top $S_c$ fingerprints
of global index with size $S_g$ is:
\begin{equation}
  E = (S_c / S_g)^{1-\alpha} \;  for\;  \alpha<1
\end{equation}
%[Describe the dedup efficiency model in detail]
Let $N$ be the number of nodes in the cluster, $m$ be the memory on each node that are used by PDS, $D$ be the amount of data on each node, and $B$ be the average block size. Then $S_c$ and $S_g$ can be expressed as:
\begin{equation}
S_c = N*m/F, \; S_g = N*D/B
\end{equation}
By replacing $S_c$ and $S_g$ in the first formula, the deduplication efficiency becomes:
\begin{equation}
  E = (\frac{m*B}{F*D})^{1-\alpha}
\end{equation}
Since $B$, $D$ and $F$ are pre-configured constants, the deduplication efficiency of PDS is only controlled by the its memory usage.

\begin{table}
    \begin{tabular}{llll}
    Data size (GB) & 1\%    & 2\%    & 4\%    \\
    14.6           & 18.6\% & 22.1\% & 31.4\% \\
    28.1           & 19.5\% & 26.2\% & 38.8\% \\
    44.2           & 21.7\% & 26.5\% & 36\%   \\
    61.6           & 23.2\% & 32.9\% & 35\%   \\
    74.2           & 23.6\% & 33.6\% & 37.5\% \\
    \end{tabular}
    \caption{Deduplication effectiveness of top k\% of global index}
    \label{tab:cds}
\end{table}

}

\subsection{ VM-centric Approximate Snapshot Deletion with Leak Repair}

\begin{figure}[htbp]
  \centering
  \epsfig{file=images/deletion.png, width=3.5in}
  \caption{Approximate deletion}
  \label{fig:deletion_flow}
\end{figure}

In a busy VM cluster, snapshot deletions are as frequent as snapshot creations.
However, data deduplication complicates the deletion process because space saving relies on the sharing of data,
thus making it difficult to decide which chunks are safe to delete.
In a traditional deduplication system, deletion often require looking at global scope to
resolve the data dependency between logical backup entities and physical data chunks.
Our VM-centric snapshot storage design simplifies the deletion process since 
we only need to locate unreferenced chunks within each VM's snapshot store to free up space when deleting a snapshot.
The PDS data chunks are commonly shared all VMs and we do not consider their reference
counting during snapshot deletion.
%The selection of PDS data chunks is updated periodically independent of snapshot deletion process.

While we can use the standard mark-and-sweep technique~\cite{mark-sweep}, 
it still takes significant time to conduct this process every time there is a snapshot deletion.
In the case of Alibaba, snapshot backup is conducted automatically and there are 
about 10 snapshot stored for every user. When there is
a new snapshot created every day,  there will be  a snapshot expired everyday to maintain
a balanced storage use. Given the large number of snapshot deletion requests, we seek
a fast solution with a very low resource usage to delete snapshots.

With this in mind, we develop an {\em approximate} deletion strategy to trade deletion accuracy for
speed and resource usage. Our method sacrifices a small percent of storage leakage
to effectively identify unused chunks in $O(n)$ speed, with $n$ being the logical number of non-PDS chunks 
to be deleted from a VM snapshot store.
The algorithm contains three aspects.


%Our system adopts VM-centric snapshot  lazy delete strategy so that all snapshot deletions are scheduled
%in the backup time window at midnight. 
%Therefore, snapshot deletions must be fast enough to fit in time window and
%efficient enough to satisfy our resource constraints.
%However, there is no simple solution can achieve these goals with high reliability.
%Our hybrid deletion strategy, using fuzzy deletion regularly and accurate deletion periodically,
%accomplishs our speed, resource usage and relibility goals very well.
\begin{itemize}
\item {\bf Computation for snapshot fingerprint summary.}
Every time there is a new snapshot created,
we compute a bloom-filter with $z$ bits as the summary of reference pointers for all non-PDS chunks used 
in this snapshot. 

%To control the false positive ratio $\epislon$ under $0.01$, an average snapshot of size 40 $GB$ with 
%$u \approx 10$ million chunks, $z$ has 10 million bits. 

%{\bf Creating bloom filter} Scan all the living snapshot recipess and their segment recipes,
%for every reference pointing to append store, add it to the bloom filter.

\item {\bf Approximate deletion with fast summary comparison.}
Then when there is a snapshot deletion,  
we need to identify if  chunks to be deleted from one snapshot
are still used by other snapshots. 
This is done approximately and quickly by comparing the reference pointers of deleted snapshots with
the merged reference bloom-filter summary of other live snapshots.
The merging of live snapshot bloom-filter bits uses the logical OR operator and the merged vector still takes $z$ bits.
Since the number of live snapshots is limited for each VM (e.g. 10 in the Alibaba's production system), 
the time cost of this comparison is small.
%Instead of scanning the entire append store indices, we merge the type-1 summaries of all
%valid snapshots. 
%Since each VM has uniform bloom filter parameters to create snapshot summaries, 
%such merged summeries give us a compact representation of
%all block fingerprints that are still in use.
%Thus by the property of bloom filter, if a fingerprint is not found in merged summaries, 
%we are certain that block is no longer used by any valid snapshot, it would be then added
%to append store's deletion log.
However, there is a small false-positive ratio which
would identify unused data as in use, resulting in temporary storage leakage.


\item {\bf Periodic repair of leakage}
%[exlpain why second bloom filter, why scan append store]
Since there are certain unused chunks which are not deleted during
the approximate deletion, leakage repair is conducted periodically.
In this phase we load the reference pointers of all chunks from a VM snapshot store as a table in memory,
then scan through all the snapshots' segment recipes. For each reference pointer that is in use by a snapshot,
we mark the corresponding entry in the table as in use. Finally those unmarked (and therefore unused) reference pointers in the
table represent chunks that can be safely deleted.

The cost of leak repair mainly come from holding a table of reference pointers and scan of all snapshots metadata.
Consider each reference pointer consumes 8 bytes plus 1 byte as mark field, a VM that has 40GB backup data with about
10 million chunks will need 90MB of memory to construct such a table. 
Also, scanning all snapshots metadata is many times slower comparing
to the approximate deletion which only scans single snapshot's metadata.
It's worth to mention that compare to previously-developed mark-and-sweep techniques, 
our leak repair still has advantage because the scope of repair is restricted within single VM's snapshot 
store due to our VM-specific design, and we only do it periodically.
%We cannot simply repeat the phase 1 multiple times to reduce temporary storage leakage, because:
%\begin{enumerate} 
%\item After several runs of phase-1, it is proven that the merged type-1 summaries cannot sieve remaining unused blocks, due to the false-positive property of bloom filter.
%\item The recipes of deleted snapshots have been removed from the system, thus we are not able to obtain the deleted block fingerprints from any metadata, the only way to discover them is to scan the append store indices.
%\end{enumerate}
\end{itemize}




%\item {\bf Check existance} For every data reference in the deleted snapshot recipe and its segment recipes,
%check the existance of that data reference in bloom filter. If not found, it is safe to delete that piece of data from append store
%because no living snapshots has referenced it.

%The overall time of running a approximate deletion for one snapshot deletion would be scanning
%all the living snapshots and deleted snapshots, since operations on the in-memory bloom filter can be done in
%parallel and is much faster than loading recipes from DFS:
%\begin{equation}
%T = (N_{SS} + 1) * T_{scan\_recipes}
%\end{equation}
%
%Using the example and analysis in previous section, this approximate deletion can be done in 5 minutes.
%Memory usage of the bloom filter depends on its false-positive probility $P_{bl}$,
%when set $P_{bl}$ to 0.01, the memory footprint of approximate deletion is about 15 MB.
%~



%{\bf Discussion}
We now estimate the storage leakage and how often leak repair needs to be conducted.
Assuming a VM always maintain $h$ snapshots in the backup, and it creates and deletes one snapshot
everyday. Let $u$ be the total number of chunks brought by the initial backup, $\Delta u$ be the average
number of additional unique chunks added from one snapshot to the next snapshot. Then the total number of unique
chunks used is about:
\[
U = u + (h-1)\Delta u.
\]

Each bloom filter vector has  $z$ bits for each snapshot and let $j$ be the number of hash functions used by the
bloom filter. The probablity that a particular bit is still in all $h$ summary vectors is still 0 is 
$(1- \frac{1}{z}) ^{j U}$. Notice that when a chunk appears multiple times in these summary vectors, it does not 
increase the probability of a particular bit being 0 in all $h$ vectors.
Then the false-positive rate $\epsilon$ is: 
\[
\epsilon = (1-(1-\frac{1}{z})^{jU})^j.
\]

For each snapshot deletion, the amount of chunks need to be deleted is nearly identical to the number of
newly add chunks $\Delta u$. However, some of chunks among them are not detected as unused in our approximation
algorithm, thus forms the storage leakage. Let $R$ be the total number of runs of approximate deletion since
last repair. we estimate  the total leakage $L$ after $R$ runs as:
\[
L = R \Delta u \epsilon
\]

When leakage ratio $L/U$ exceeds a pre-defined threshold $\tau$, we need to execute a leak repair:
\[
\frac{L}{U} = \frac{R \Delta u \epsilon}{u+(h-1)\Delta u } > \tau 
\Longrightarrow R > \frac{\tau}{\epsilon}\frac{u + (h-1)\Delta u}{\Delta u}
\]
For example in our tested dataset,  
each VM keeps $h=10$ snapshots and each snapshot has
about 1-5\% of new data. Thus $\frac{\Delta u}{u}=0.05$ at most. With 40GB snapshot size, $u\approx = 10$ millions.
Then $U=10.45$ millions.
We choose  $\epsilon = 0.01$ and $\tau=0.1$. Then from the bloom filter  false positive formula, 
$z=10U=100.45$ million bits. $R=290$.
Then we would expect leak repair be triggered once for 
every 290 runs of approximate deletion. 
When one machine hosts 25 VMs and there is one snapshot deletion per day per VM, there would be 
only one full leak repair for one VM scheduled for every 12 days. Each repairs uses about 90MB memory on average
as discussed earlier and takes a short period of time.
% which is sufficiently small to offset the heavy I/O workload of the mark-sweep process.
