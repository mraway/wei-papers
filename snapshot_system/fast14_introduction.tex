\section{Introduction}
In a cluster-based cloud environment,
each physical machine runs a number  of virtual machines as  instances of a guest operating system 
and their  virtual hard disks are represented as virtual disk image files in the host operating system.
Frequent  snapshot backup of virtual disk images  can increase  the service reliability. 
For example, the Aliyun cloud, which is  the largest cloud service provider by Alibaba in China, 
automatically conducts  the backup of virtual disk images to all active users every day.
The cost of supporting a large number of concurrent backup streams is high
because of the huge storage demand. 
Using a separate  backup service with full deduplication support~\cite{venti02,bottleneck08}
can effectively identify and remove content duplicates among snapshots, 
but such a solution can be expensive. There is also a large amount of 
network traffic to transfer  data from the host machines to the backup facility
before duplicates are removed.

This paper seeks for a low-cost architecture option  that collocates
a backup service with other cloud services and  uses a minimum amount of resources. 
We also consider the fact that after
deduplication, most data chunks are shared by several to many virtual machines.
Failure of a few shared data chunks can have a 
broad effect and many
snapshots of virtual machines could be affected.
The previous work in deduplication focuses on the efficiency and approximation of
fingerprint comparison, and has not addressed fault tolerance issues  together with deduplication.
Thus we also seek deduplication options that yield better fault isolation.
Another issue considered is that
that garbage collection after deletion of old snapshots also competes for computing resources. 
Sharing of data chunks among by multiple VMs needs to be detected during
garbage collection and such dependencies complicate deletion operations. 

The key contribution of this work is the development and analysis of a VM-centric approach
which considers fault isolation and integrates  multiple duplicate detection strategies
supported by similarity guided local deduplication
and popularity guided global deduplication. 
This approach localizes duplicate detection within each VM  
and packages only data chunks from the same VM into a file system block as much as possible.
By narrowing duplicate sharing within a small percent of common data chunks and exploiting their popularity,
this scheme can afford to allocate extra replicas of these shared chunks for better
fault resilience while sustaining competitive deduplication efficiency.
In addition, our VM-centric design allows garbage collection to be performed in a localized
scope and we propose an approximate deletion scheme to reduce this cost further.
Localization also brings the benefits of greater ability to exploit parallelism so
backup operations can run simultaneously without a central  bottleneck.
This VM-centric solution uses a small amount of memory with only a slight tradeoff in deduplication efficiency. 

We have developed a prototype system that runs a cluster of Linux machines with Xen and uses 
a standard distributed file system for the backup storage. 

%************** Paper sections summary
%THIS NEEDS MODIFICATION
The rest of this paper is organized as follows.
Section~\ref{sect:background} reviews the background and discusses the  design options for snapshot backup 
with a VM-centric approach. 
Section~\ref{sect:deduplication}  analyzes the tradeoff and benefits of our approach. 
Section~\ref{sect:architecture}  describes our system architecture and implementation details.
%   the benefit of our approach for fault isolation. 
Section~\ref{sect:evaluation} is our experimental evaluation that compares with other approaches.
Section~\ref{sect:conclusion}  concludes this paper.
