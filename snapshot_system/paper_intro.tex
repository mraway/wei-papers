\section{Introduction}

One of emerging architectures for building cloud services
is a converged storage architecture that leverages commodity servers with software clustered storage.
This converged architecture (server plus compute in same tier) grew out of the Google model. 
What Google recognized is that they could leverage their software to 
meld multiple direct attached low-cost disks together across servers, 
such approach allow Google to avoid paying high margins to traditional network attached storage vendors,
as well as ditching their scalability limitations. 

Server virtualization is one of the most benefited area of adopting converged storage architecture.
Leading cloud platform providers such as Google AppEngine, Amazon AWS, 
Microsoft Azure and Alibaba have built such a converged compute and storage infrastructure and used
a distributed file system such as Google file system~\cite{googlefs03,hdfs10}
to glue a large number of commodity servers.
And such converged storage architecture also becomes increasingly popular 
in VDI environment in private clouds
provided by Nutanix~\cite{NutanixPaper}, Simplivity and others. 
% with local storage into a single cluster.
In such an environment,
%That  allows the use of scalable compute and storage, without incurring the costs and
%performance limitations associated with network storage.
each physical machine runs a number  of virtual machines 
%as  instances of a guest operating system 
and their virtual disks are represented as virtual disk image files in the host operating system.

As more and more production 
applications are running in virtualized environments in the cloud,
the need to protect these virtual machines against data loss is urgent.
Frequent snapshot backup of virtual disk images  can increase  the service reliability
and deduplication of redundant content blocks~\cite{venti02,bottleneck08}
 is necessary to substantially reduce the storage demand.
This is typically done through offloading backup workload from production 
server hosts and consolidating it on dedicated backup proxy servers (e.g. EMC Data Domain) or
backup services (e.g. Amazon S3). This approach simplifies the cluster architecture and 
avoids potential performance hits to the production applications when backup is in progress,
but it also brings its own problems:
From VM users' perspective, the cost of a dedicated backup storage is very high.
From cloud architecture's perspective, sending out pre-deduplication backup data wastes huge amount
of bandwidth. Finally and most importantly, such dedicated backup storage is either difficult to scale out
which would comprise the overall scalability of the converged architecture, 
or has to give up global deduplication in order to gain scalability which would then increase the backup cost.

Contrary to using dedicated backup storage for VM snapshots, 
in this paper we propose a collocated backup storage sitting on top of 
the existing distributed file system in a converged storage architecture,
and provide methodologies to address associated challenges: 
First, it must achieves good global deduplication 
while the computing resources are very limited and spread over every node. 
Second, it has to provide augmented fault isolation for the sharing of deduplicated data,
given that node failures in converged architecture are the norm rather than exception.
Finally, it needs to provide easily managable garbage collection so that user immediately
sees storage charge changes after he deletes a snapshot.

% low-cost VM-centric snapshot backup approach that is collocated within 
% the existing converged storage architecture. Our solution leverages the scalability of the underlying
% distributed file system and provides global data deduplication in a resource friendly manner.

%For example, the Aliyun cloud, which is  the largest cloud service provider by Alibaba in China, 
%automatically conducts  the backup of virtual disk images to all active users every day.
%The cost of supporting a large number of concurrent backup streams is high
%because of the huge storage demand and the use of deduplication
 
%Using a separate  backup service with full deduplication support~\cite{venti02,bottleneck08}
%can effectively identify and remove content duplicates among snapshots, 
%but such a solution can be expensive. There is also a large amount of 
%network traffic to transfer  data from the host machines to the backup facility
%before duplicates are removed.

While version-based detection has been used to identify file blocks that have not 
changed from the previous version of the snapshot~\cite{Clements2009,Vrable2009,TanIPDPS2011},
a popular technique for deduplicatioon is to 
conduct fingerprint  comparison and identify duplicates that exist
among all files~\cite{Guo2011,Dong2011,extreme_binning09}. 
Because of highly repetitive content in snapshots from different VMs,
many data chunks are shared by virtual machines.  
Failure of a few shared data chunks can have a 
broad effect and snapshots of these virtual machines could be affected.
The previous work in deduplication focuses on the efficiency and approximation of
fingerprint comparison, and has not addressed fault tolerance issues  together with deduplication.
Thus we seek a method that strikes a balance between fault isolation and deduplication efficiency.

The main contribution of this work is to evaluate the fault tolerance  impact of popular data blocks shared by many
virtual machines, and   propose a VM-centric approach that localizes deduplication as much as possible 
and restricts global deduplication only to a limited set of most popular blocks.
Local deduplication also uses similarity-guided elimination to improve the deduplication coverage.
Since the file system block size is normally bigger than the average data chunk size used for deduplication,
we package data chunks from the same VM into a file system block as much as possible to improve fault isolation.
Because data sharing is restricted, 
this VM-centric approach reduces the overall resource usage significantly during backup and
simplifies the snapshot deletion process. This low-resource design
is suitable when the backup service with deduplication is collocated with other services running on a shared compute and storage
cluster.  We have evaluated this VM-centric approach using  a prototype system.
% that runs a cluster of Linux machines with Xen and a standard distributed file system for the backup storage. 
%This approach localizes duplicate detection within each VM  
%By narrowing duplicate sharing within a small percent of common data chunks and exploiting their popularity,
%this scheme can afford to allocate extra replicas of these shared chunks for better
%fault resilience while sustaining competitive deduplication efficiency.


%In addition, our VM-centric design allows garbage collection to be performed in a localized
%scope and we propose an approximate deletion scheme to reduce this cost further.
%Localization also brings the benefits of greater ability to exploit parallelism so
%backup operations can run simultaneously without a central  bottleneck.
%This VM-centric solution uses a small amount of  memory while delivering reasonable deduplication efficiency. 

%Another issue considered is that
%that garbage collection after deletion of old snapshots also competes for computing resources. 
%Sharing of data chunks among by multiple VMs needs to be detected during
%garbage collection and such dependencies complicate deletion operations. 

%************** Paper sections summary
%THIS NEEDS MODIFICATION
The rest of this paper is organized as follows.
Section~\ref{sect:background} reviews the background and discusses the  design options for snapshot backup 
with a VM-centric approach. 
Section~\ref{sect:deduplication}  analyzes the tradeoff and benefits of the VM-centric approach. 
Section~\ref{sect:architecture}  describes a system implementation that evaluates the proposed techniques.
%   the benefit of our approach for fault isolation. 
Section~\ref{sect:evaluation} is our experimental evaluation that compares with other approaches.
Section~\ref{sect:conclusion}  concludes this paper.
