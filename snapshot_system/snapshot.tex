\section{Snapshot Deduplication}
\label{sect:dedupe}
%snapshot representation

%We use the The deduplication process is first conducted among snapshots within each VM
%and then is conducted across VMs.
%Given the concern that searching duplicates across VMs is a global feature which can affect parallel performance
%and complicate failure management,
%we only eliminate the duplication of a small but popular data set while still maintaining a cost-effective deduplication ratio.
%For this purpose, we exploit the data characteristics of snapshots and collect most popular data.
%Data sharing across VMs is limited within this small data set such that adding replicas for it could enhance fault tolerance

Our  deduplication scheme compares the fingerprints of the current snapshot
with its parent snapshot and also other snapshots in the entire cloud.
This process performs   the duplication in two categories: \textit{Inner-VM} and \textit{Cross-VM}. 
We discuss the integration of several strategies as follows and analyze its effectiveness.

\subsection{Inner and cross VM deduplication}

Inner-VM duplication can be effecive within  VM's snapshots because the majority of data are unchanged during each backup period. 
Localizing the snapshot data deduplication witin  a VM first
increases data independency between different VM backups,
simplifies snapshot management and statistics collection,
and facilitates parallel execution of snapshot operations.
On the other hand, cross-VM duplication can also be desirable mainly due to widely-used software and libraries. 
As the result, different VMs tend to backup large amount of highly similar data.
We integrate those strategies together as follows.
%Our multi-level pipeline process can minimize 
%the cost of deduplication while maximize the its efficiency at each level,
%and it is parallel since each segment is processed independently.

\begin{itemize}
\item \textbf{Dirtybit-based corase-grain inner-VM deduplication.}
The first-level deduplication is to follow the standand dirty bit approach, but is conducted
in the coarse grain segment level.
We use the  Xen virtual device driver which supports changed block tracking
for the storage device
and the dirty bit setting is maintained in a corase grain level we call it a segment.
In our implementation, the segment size is 2MB. 
Since every write for a block will touch a dirty bit, the device driver maintains dirtybits in memory
and cannot affort a small segment  size.

It should be noted that changed block tracking is supported or can be easily implemented in several 
major virtualization solution vendors. 
%(VMware, Xen, Microsoft).  
VMware support it directly. Their hypervisor has API to let external backup application know 
the changed areas since last backup.  Xen doesn't directly support it. However, their open-source 
architecture allows anyone to extend the device driver, thus enabling changed block tracking. 
We implements this way in the Alibaba's platform.  
Microsoft SDK provides an API that allows external application monitor 
the VM's I/O traffic.  Therefore changed block tracking can be implemented externally. 

\item \textbf{Chunk-level fine-grain nearby duplicate detection.}
The best deduplication uses a nonuniform chunk size 
in the average of 4K or 8K~\cite{??}.
Thus the second-level inner-VM deduplication is to assess in this
level, but only for those dirty  segments. 
We load the fingerprints of block chunks of the corresponding segment from the
parent for a comparisons and compare near-by fingerprint matching within this segement.
The amount of memory for maintaining those fingerprints  is small.
For example, with a 2MB segment, there are about 500 fingerprints to compare.


%If we use 4KB in level-1, then such a level-1 should have similar dedup efficiency 
%as the current level-1 and level-2 combined, because finally they equal to comparing  
%parent snapshot at 4KB granularity.
%
%However, at level-3 things are different. If we use 4KB fix-size block uniformly, it would be harder for different VM to share data through CDS, because there is no guarantee that the location of duplicate data on different VM disks are always aligned at 4KB boundary. For example, if two VMs each has a copy of duplicate data, but they are not aligned, then we won't be able to detect them. Our study at the current small data set has shown that using 4KB fix-size block will make CDS method less efficient by nearly 10%. Over the long time, more and more OS variations will co-exist in the cluster, making this 4KB fix-size approach inefficient in reducing duplicate data across VMs.

%\item \textit{Level 2  Chunk fingerprint comparison.}
%If a segment is modified, we perform fine-grained deduplication 
%by comparing the fingerprints of its chunks to the same segment's recipe in the previous snapshot,
%thus eliminate partial data duplication within the segment.
%\end{itemize}
%
%In general, operations at level 1 have almost no cost and most of unmodified data are filtered here. 
%To process a dirty segment at level 2, 
%there requires no more than one DFS access to load the segment recipe from previous snapshot,
%and a tiny amount of memory to hold it in main memory.

\item \textbf{Cross-VM deduplication.}
This step accomplishes the standand global fingerprint  comparison as conducted
in the previous work~\cite{??}.
One key observation is that the inner deduplication has removed many of duplicates.
There are not lot of deduplication opportunities cross VMs while the memory
consumption for global comparison is expensive.
Thus our approximation is that the global fingerprint  comparison  only searches for the most popular items. 
%Since duplicate sharing patterns among  VM follows a zip-like distribution,  
\end{itemize}

\subsection{Popular Chunk Management and VM-oriented Fault Isolation}
Our objective for fault isolation is to minimize the number of VMs affected when there are failures
in the cluster.  The inner-VM deduplication does not depend on any global service and the comparison
for each VM is localized within the parent and the current snapshot.
Thus there is no data dependence between VMs.
For cross-VM deduplication, there is a data dependence among VMs and we would minimize the faiulre impact
of shared blocks by adding extra replcas of those shared blocks.

This section analyzes the choice of popular blocks and its impact on the deduplication efficiency.
It also  compares the  fault resilience of our VM-centric deduplication approach with a standand approach using 
global deduplication.

\subsubsection{Impact of CDS deduplication}

Our empirical study based on VM images from production environment\cite{ieeecloud} showed that the
frequency of data duplication follows Zipf-like distribution\cite{zipf},
with the exponent $\alpha$ between 0.65 ~ 0.70.
As a result, it can be proved that deduplication efficiency of CDS index is scalable:

Let $b$ be the total amount of data blocks after localized deduplication, $b_u$ be the total amount of fingerprints 
in the global index after complete deduplication, $C_i$ be the frequency for the $i$th most popular fingerprint. 
By Zipf-like distribution we know an fingerprint's frequency is inversely proportional to its rank:
\begin{equation*}
C_i = X\frac{1}{i^\alpha},\; \sum_{i=1}^{b_u}C_i = X\sum_{i=1}^{b_u}\frac{1}{i^\alpha} = b
\end{equation*}

Thus $C_i$ can be expressed as:
\begin{equation}
C_i=\frac{b(1-\alpha)}{b_u^{1-\alpha}i^\alpha}
\end{equation}

We define deduplication efficiency $e$ to be the proportion of raw data that can be deduplicated,
In complete deduplication scenario, it deduplication efficiency $e_c$ is:
\begin{equation}
e_c = \frac{b-b_u}{b} = 1 - \frac{b_u}{b}
\end{equation}

If we only select the top $k$ most popular fingerprints from global index and remove only their duplicates,
the deduplication efficiency will be:
\begin{equation}
\begin{split}
e_k &= (\sum_{i=1}^{k}C_i - k)/{b} = (b * \frac{1-\alpha}{b_u^{1-\alpha}} * \sum_{i=1}^{k}\frac{1}{i^\alpha} - k)/b \\
%&\approx \frac{b*\frac{1-\alpha}{b_u^{1-\alpha}}*\frac{k^{1-alpha}}{1-\alpha} - k}{b}
&\approx (\frac{k}{b_u})^{1-\alpha} - \frac{k}{b} \approx (\frac{k}{b_u})^{1-\alpha},\; (since\; k \ll b)
\end{split}
\end{equation}

% For the Zipf-like distribution, an approximation to the sum of the first $n$ 
% elements of the distribution can be derived as follows:
% \begin{equation}
% \sum_{i=1}^{n}\frac{1}{i^\alpha}\approx \int_{1}^{n}\frac{1}{x^\alpha}\mathrm{d}x=\frac{x^{1-\alpha}}{1-\alpha}=\frac{n^{1-\alpha}}{1-\alpha}\;  for\;  \alpha<1
% \end{equation}

% So the cumulative distribution function for a CDS holding top $S_c$ fingerprints
% of global index with size $S_g$ is:

% \begin{equation}
%   E = (S_c / S_g)^{1-\alpha} \;  for\;  \alpha<1
% \end{equation}

%[Describe the dedup efficiency model in detail]
Let $N$ be the number of nodes in the cluster, $m$ be the memory on each node used by CDS, 
$F$ be the size of an CDS index entry,
$D$ be the amount of unique data on each node, and $B$ be the average block size. Then $k$ and $b_u$ can be expressed as:
\begin{equation*}
k = N*m/F,\; b_u = N*D/B
\end{equation*}

Then the deduplication efficiency of CDS approach using top k most popular fingerprints is:
\begin{equation}
  e_k = (\frac{m*B}{F*D})^{1-\alpha}
\end{equation}

Since $B$, $D$ and $F$ are environment constants, the deduplication efficiency of CDS is mainly affected by its memory usage,
and the Zipf-like distribution parameter $\alpha$.
From the memory usage perspective, the more memory we give to CDS index, the more duplicates it can found.
From the data characteristics perspective, if the system tends to backup arbitrarily random data, 
$\alpha$ would be small and thus CDS approach would not be effective. But in a VM cluster all VMs
have a large portion of similar data, thus leading to a bigger $\alpha$. The more VM join the cluster,
the better deduplication efficiency our CDS deduplication approach will have.


