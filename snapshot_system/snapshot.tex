\section{VM-centric Snapshot Deduplication}
\label{sect:dedupe}
%snapshot representation

%We use the The deduplication process is first conducted among snapshots within each VM
%and then is conducted across VMs.
%Given the concern that searching duplicates across VMs is a global feature which can affect parallel performance
%and complicate failure management,
%we only eliminate the duplication of a small but popular data set while still maintaining a cost-effective deduplication ratio.
%For this purpose, we exploit the data characteristics of snapshots and collect most popular data.
%Data sharing across VMs is limited within this small data set such that adding replicas for it could enhance fault tolerance

A deduplication scheme compares the fingerprints of the current snapshot
with its parent snapshot and also other snapshots in the entire cloud.
The traditional approach compares all fingerprints and 
stores one copy for all of a block's duplicates.
We call this as  the VM-oblivious (VO) approach.
In comparison, we analyze the design of
a VM-centric approach (VC) which differenates duplicates within a VM and cross VMs,
and conducts \textit{Inner-VM} and \textit{cross-VM} detection seperately. 

\subsection{Inner and cross VM deduplication}

Inner-VM duplication can be very effecive within VM's snapshots. There
are typically a large number of duplicates existing among the snapshots
for a single VM.  Localizing the snapshot data deduplication witin a VM
improves the system by:
increasing data independency between different VM backups,
simplifying snapshot management and statistics collection,
and facilitating parallel execution of snapshot operations.
On the other hand, cross-VM duplication can also be desirable mainly due to widely-used software and libraries. 
As the result, different VMs tend to backup large amount of highly similar data.
We integrate those strategies together as follows.
%Our multi-level pipeline process can minimize 
%the cost of deduplication while maximize the its efficiency at each level,
%and it is parallel since each segment is processed independently.

\begin{itemize}
\item \textbf{Dirtybit-based corase-grain inner-VM deduplication.}
The first-level deduplication is to follow the standand dirty bit approach, but is conducted
in the coarse grain segment level.
We use the  Xen virtual device driver which supports changed block tracking
for the storage device
and the dirty bit setting is maintained in a corase grain level we call it a segment.
In our implementation, the segment size is 2MB. 
Since every write for a block will touch a dirty bit, the device driver maintains dirtybits in memory
and cannot affort a small segment  size.

It should be noted that changed block tracking is supported or can be easily implemented in many 
major virtualization solution vendors. 
%(VMware, Xen, Microsoft).  
VMware support it directly; the VMWare hypervisor has an API to let external backup application know 
the changed areas since last backup. Xen doesn't directly support it, However, their open-source 
architecture allows anyone to extend the device driver, thus enabling changed block tracking. 
We implement dirty bit tracking this way in Alibaba's platform.  
The Microsoft SDK provides an API that allows external applications to monitor 
the VM's I/O traffic, therefore changed block tracking can be implemented externally. 

\item \textbf{Chunk-level fine-grain inner-VM detection.}
The best deduplication uses a nonuniform chunk size 
in the average of 4K or 8K~\cite{??}. This allows the system to
achieve deduplication even when there are insertions/deletions which would
affect many fixed-size blocks.
Thus the second-level inner-VM deduplication is to perform this chunking for
dirty segments, and to compare the snapshot with its parent. 
We load the fingerprints of block chunks of the corresponding segment from the
parent and perform fingerprint matching for further inner-VM deduplication.
The amount of memory for maintaining those fingerprints  is small, as we only
load one segment at a time.
For example, with a 2MB segment, there are about 500 fingerprints to compare.


%If we use 4KB in level-1, then such a level-1 should have similar dedup efficiency 
%as the current level-1 and level-2 combined, because finally they equal to comparing  
%parent snapshot at 4KB granularity.
%
%However, at level-3 things are different. If we use 4KB fix-size block uniformly, it would be harder for different VM to share data through CDS, because there is no guarantee that the location of duplicate data on different VM disks are always aligned at 4KB boundary. For example, if two VMs each has a copy of duplicate data, but they are not aligned, then we won't be able to detect them. Our study at the current small data set has shown that using 4KB fix-size block will make CDS method less efficient by nearly 10%. Over the long time, more and more OS variations will co-exist in the cluster, making this 4KB fix-size approach inefficient in reducing duplicate data across VMs.

%\item \textit{Level 2  Chunk fingerprint comparison.}
%If a segment is modified, we perform fine-grained deduplication 
%by comparing the fingerprints of its chunks to the same segment's recipe in the previous snapshot,
%thus eliminate partial data duplication within the segment.
%\end{itemize}
%
%In general, operations at level 1 have almost no cost and most of unmodified data are filtered here. 
%To process a dirty segment at level 2, 
%there requires no more than one DFS access to load the segment recipe from previous snapshot,
%and a tiny amount of memory to hold it in main memory.

\item \textbf{Cross-VM deduplication.}
This step accomplishes the standand global fingerprint  comparison as conducted
in the previous work~\cite{??}.
One key observation is that the inner deduplication has removed many of the duplicates.
There are fewer deduplication opportunities across VMs while the memory and network
consumption for global comparison is more expensive.
Thus our approximation is that the global fingerprint comparison only searches for the most popular items. 
%Since duplicate sharing patterns among  VM follows a zip-like distribution,  
\end{itemize}

When a block is not detected as a duplicate to any existing block, this block will be written
to the file system. Since the backend file system typically uses a large block size such as 64MB, each VM will 
accululate small local blocks. We manage this accumulation process using an apendstore  scheme
and discuss this in details in Section~\ref{sect:arch}.


 \subsection{Popular Chunk Management}
%VM-oriented Fault Isolation}
%Our objective for fault isolation is to minimize the number of VMs affected when there are failures
%in the cluster.  
%The inner-VM deduplication does not depend on any global service and the comparison
%for each VM is localized within the parent and the current snapshot.
%Lack of data dependence between VMs brings 
With cross-VM deduplication, shared data blocks
create an artifical dependence  among VMs and failure of one shared block
affects many VMs. Thus we only maintain the index for a small set of popular chunks.
The management for popular data chunks contains two aspects.
\begin{itemize}
\item Compute and select top-$k$ most popular chunks.
The popularity of a chunk is the number  of data blocks from different VMs
that are duplicates of this block after the inner VM deduplication.
This number can be computed perioidically in a weekly basis.
Once the popularity of all data blocks is collected, the system only maintains the top $k$
most popular blocks.  For cross-VM comparison,
we only store  top $k$ items and $k$ is chosen to be relatively small.
Our analysis below will show that the algorithm can still deliver competitive
deduplication effiency after making these approximations.
\item  Since $k$ is small and these top $k$ blocks are shared among multiple VMs, 
we can afford to provide extra replicas for these popular blocks to enhanace the fault resilence.
We will provide an analysis of the fault tolerance in the next subsection.
\end{itemize} 

This section analyzes how value   $k$ impacts the deduplication efficiency.
%It also  compares the  fault resilience of our VM-centric deduplication approach with a standand approach using 
%global deduplication.
%\subsubsection{Impact of CDS deduplication}
The analysis is based on VM traces collected
using Alibaba's production user data~\cite{ieeecloud} 
which shows that the popularity of data blocks after inner VM deduplication follows 
a Zipf-like distribution\cite{zipf} and its
exponent $\alpha$ is ranged between 0.65  and  0.70.
%As a result, it can be proved that deduplication efficiency of CDS index is scalable:

\begin{table}[ht]
\centering
\begin{tabular}{|p{1.25cm}|p{6.5cm}|}
\hline
$b$ &  the total amount of data blocks\\ 
\hline
$b_u$ &  the total amount of unique fingerprints after inner VM  deduplication\\
\hline
$C_i$ &  the frequency for the $i$th most popular fingerprint\\
\hline
$\delta$ &  the percentage of duplicates detected in inner VM deduplication\\
\hline
$\sigma$ &  the percentage of $b_u$ unique items  stored in CDS.\\
\hline
$p$ & the number of machines in the cluster\\
\hline
$D$ & the amount of unique data on each machine\\
\hline
$B$ & the average block size. Our setting is  4K.\\
\hline
$s$ & the average size of data containers stored in the distributed file system . Our setting is  64MB.\\
\hline
$m$ & memory size on each node used by VC\\ 
\hline
$F$ & the size of an polular data index entry\\
\hline
\end{tabular}
\caption{Modeling  symbols.}
\label{tab:symbol}
\end{table}


Let $b$ be the total number of data blocks after localized deduplication, 
$b_u$ be the total number of fingerprints 
in the global index after complete deduplication, and
$C_i$ be the frequency for the $i$th most popular fingerprint. 
By Zipf-like distribution, $C_i = \frac{C_1}{i^\alpha}.$
Since $ \sum_{i=1}^{b_u}C_i = b$,
\[
C_1 \sum_{i=1}^{b_u}\frac{1}{i^\alpha} = b
\]
Given $\alpha <1$, $C_1$ can be approximated with integration:
\begin{equation}
C_1=\frac{b(1-\alpha)}{b_u^{1-\alpha}}.
\end{equation}

%We define deduplication efficiency $e$ to be the proportion of raw data that can be deduplicated,
%In complete deduplication scenario, it deduplication efficiency $e_c$ is:
%\begin{equation}
%e_c = \frac{b-b_u}{b} = 1 - \frac{b_u}{b}
%\end{equation}

The  $k$ most popular fingerprints can cover the following number of blocks after inner VM 
deduplication.
\[
C_1 \sum_{i=1}^{k}\frac{1}{i^\alpha} \approx  
C_1 \int_{1}^{k}\frac{1}{x^\alpha} dx  \approx  C_1\frac{  k^{1-\alpha}} {1-\alpha}.
\]

Deduplication efficiency of VC using top $k$ popular blocks
is the percentage of duplicates that can be detected:  
\begin{equation}
\begin{split}
e_k &= 
\frac{ b(1-\delta) + C_1\frac{  k^{1-\alpha}} {1-\alpha}} 
{b(1-\delta)  + \delta b - b_u }\\
&= 
\frac{ (1-\delta) + \delta  (\frac{k}{b_u})^{1-\alpha}}
{ 1- \frac{b_u}{b} }.
\end{split}
\end{equation}


%\begin{equation}
%\begin{split}
%e_k &= (\sum_{i=1}^{k}C_i - k)/{b} = (b * \frac{1-\alpha}{b_u^{1-\alpha}} * \sum_{i=1}^{k}\frac{1}{i^\alpha} - k)/b \\
%&\approx \frac{b*\frac{1-\alpha}{b_u^{1-\alpha}}*\frac{k^{1-alpha}}{1-\alpha} - k}{b}
%&\approx (\frac{k}{b_u})^{1-\alpha} - \frac{k}{b} \approx (\frac{k}{b_u})^{1-\alpha},\; (since\; k \ll b)
%\end{split}
%\end{equation}

% For the Zipf-like distribution, an approximation to the sum of the first $n$ 
% elements of the distribution can be derived as follows:
% \begin{equation}
% \sum_{i=1}^{n}\frac{1}{i^\alpha}\approx \int_{1}^{n}\frac{1}{x^\alpha}\mathrm{d}x=\frac{x^{1-\alpha}}{1-\alpha}=\frac{n^{1-\alpha}}{1-\alpha}\;  for\;  \alpha<1
% \end{equation}

% So the cumulative distribution function for a CDS holding top $S_c$ fingerprints
% of global index with size $S_g$ is:

% \begin{equation}
%   E = (S_c / S_g)^{1-\alpha} \;  for\;  \alpha<1
% \end{equation}

%[Describe the dedup efficiency model in detail]
Let $p$ be the number of physical machines in the cluster, $m$ be the memory on each node used by the popular
index, $F$ be the size of an index entry,
$D$ be the amount of unique data on each phyiscal machine, and 
$B$ be the average block size. We store the popular index using a distribued shared memory hashtable
such as MemCacheD.  Then $k$ and $b_u$ can be expressed as:
$
k = p*m/F$, and $b_u = p*D/B.
$

The overall deduplication effiency of VC is
\[
\frac{ (1-\delta) + \delta (\frac{m*B}{D*F})^{1-\alpha}}
{ 1- \frac{b_u}{b} }.
\]
where $(\frac{m*B}{D*F})^{1-\alpha}$ represents the percentage of the remaining blocks detected as duplicates
after inner VM deduplication. 
%Then the deduplication efficiency of CDS approach using top k most popular fingerprints is:
%\begin{equation}
%  e_k = (\frac{m*B}{F*D})^{1-\alpha}
%\end{equation}

When the number of machines at each cluster increases, the number of total VMs increases.
Then $k$ increases since more memory is available to host the popular block index.
But for each physical machine, the number of VMs remains the same, and thus
$D$ is  a constant. Then  the overall deduplication efficiency of VC remains
a constant.

% is mainly affected by the its memory usage,
%and the Zipf-like distribution parameter $\alpha$.
%From the memory usage perspective, the more memory we give to CDS index, the more duplicates it can found.
%From the data characteristics perspective, if the system tends to backup arbitrarily random data, 
%$\alpha$ would be small and thus CDS approach would not be effective. But in a VM cluster all VMs
%have a large portion of similar data, thus leading to a bigger $\alpha$. The more VM join the cluster,
%the better deduplication efficiency our CDS deduplication approach will have.




\comments{
Since $B$, $D$ and $F$ are environment constants, the deduplication efficiency of CDS is mainly affected by its memory usage,
and the Zipf-like distribution parameter $\alpha$.
From the memory usage perspective, the more memory we give to CDS index, the more duplicates it can found.
From the data characteristics perspective, if the system tends to backup arbitrarily random data, 
$\alpha$ would be small and thus CDS approach would not be effective. But in a VM cluster all VMs
have a large portion of similar data, thus leading to a bigger $\alpha$. The more VM join the cluster,
the better deduplication efficiency our CDS deduplication approach will have.
}


\begin{table}
    \begin{tabular}{llll}
    Data size (GB) & 1\%    & 2\%    & 4\%    \\
    14.6           & 18.6\% & 22.1\% & 31.4\% \\
    28.1           & 19.5\% & 26.2\% & 38.8\% \\
    44.2           & 21.7\% & 26.5\% & 36\%   \\
    61.6           & 23.2\% & 32.9\% & 35\%   \\
    74.2           & 23.6\% & 33.6\% & 37.5\% \\
    \end{tabular}
    \caption{Deduplication effectiveness of top k\% of global index}
    \label{tab:cds}
\end{table}


\begin{figure}[ht]
  \centering
    \begin{tikzpicture}
            \begin{axis}[
            %title={CDS Coverage},
            xlabel={Total Data Stored (GB)},
            ylabel={CDS Coverage (\%)},
            %extra y ticks={4.5,5.5,6.5} %to add extra ticks
            mark options=solid,
            %legend pos=outer north east,
            legend columns=2,
            legend style={
                at={(0.5,-0.2)},
            anchor=north}
            ]
            \addplot[color=blue,mark=*] table[x=InputSize,y=A1] {figures/cds_coverage.txt};
            \addplot[color=blue,mark=square*] table[x=InputSize,y=A2] {figures/cds_coverage.txt};
            \addplot[color=blue,mark=triangle*,mark size=3] table[x=InputSize,y=A4] {figures/cds_coverage.txt};
            \addplot[densely dashed,color=red,mark=*] table[x=InputSize,y=T1] {figures/cds_coverage.txt};
            \addplot[densely dashed,color=red,mark=square*] table[x=InputSize,y=T2] {figures/cds_coverage.txt};
            \addplot[densely dashed,color=red,mark=triangle*] table[x=InputSize,y=T4] {figures/cds_coverage.txt};
            \legend{Measured (1\%CDS),Measured (2\%CDS),Measured (4\%CDS),Predicted (1\%CDS),Predicted (2\%CDS),Predicted (4\%CDS)};
            \end{axis}
    \end{tikzpicture}
  \caption{fixed-alpha predicted vs. actual CDS coverage as data size increases.}
  \label{fig-vo-links}
\end{figure}

