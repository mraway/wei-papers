\section{Introduction}

1.how virtual machines work on storage
2.cow and why they are not good
3.introduce dedup

Virtualization is the engine behind many today's cloud computing platforms.
Amazon and many others have provided clusters that host 
hundreds of thousand virtual machines(VM) which are dynamically created
by users. In such a 
each instance of a guest operating system stores a
file called a virtual disk (e.g. .vhd, .vmdk) on the host system

Because those virtual machines are stored in  maVirtualization provide

Backing user data on these virtual disks is quite different from traditional file system backup.
 
File storage is a critical component in cloud infrastructure, because users and applications expect a  
persistent place to store and share their data. 
But there can be a significant amount of data redundancy in the storage system
 because large number of users employ such systems for backups, 
and file data between different users may overlap especially when user base is huge. 
For example, in a VM-based cloud, users usualy backup their virtual machine images along with application data 
to a cloud file storage periodically. 
Therefore, improving the space efficiency is an 
important factor in designing a file storage service for cloud.

Data deduplication technique can eliminate such redundancy,
today many D2D backup systems\cite{emc_avamar}\cite{datadomain_whitepaper}.
uses vaiable-size chunking algorithm to detect duplicates in file data.
Chunking divides a data stream into variable length chunks, it has been used to conserve bandwidth\cite{lbfs01}, 
search and index documents in large repositories\cite{bhag07}, scalable storage systems\cite{hydrastor09}, 
store and transfer large directory trees efficiently and with high reliability\cite{jumbo07}.

To chunk a file, starting from the first byte, its contents as seen through a fixed-sized (overlapping) 
sliding window are examined. At every position of the window, a fingerprint or signature of its 
contents, $f$, is computed using hashing techniques such as Rabin fingerprints\cite{rabin81}. 
When the fingerprint meets a certain criteria, such as $f mod D = r$ where $D$, the divisor, 
and $r$ are predefined values; that position of the window defines the boundary of the chunk. 
This process is repeated until the complete file has been broken down into chunks. Next, 
a cryptographic hash or chunk ID of the chunk is computed using techniques such as MD5 or SHA.
After a file is chunked, the index containing the chunk IDs of backed up chunks 
is queried to determine duplicate chunks. New chunks are written to disk and the 
index is updated with their chunk IDs. A file recipe containing all the information 
required to reconstruct the file is generated. The index also contains some metadata 
about each chunk, such as its size and disk location.
How much deduplication is obtained depends on the inherent content overlaps in the data, 
the average size of chunks and the chunking method\cite{poli04}. In general, smaller chunks yield better deduplication.

Inline deduplication is deduplication where the data is deduplicated before 
it is written to disk as opposed to post-process deduplication where backup data 
is first written to a temporary staging area and then deduplicated offline. 
One advantage of inline deduplication is that extra disk space is not required to 
hold and protect data yet to be backed up. 
However, unless some form of locality or similarity is exploited, inline, chunk-based deduplication, 
when done at a large scale faces what has been termed the disk bottleneck problem: to facilitate fast chunk ID lookup, 
a single index containing the chunk IDs of all the backed up chunks must be maintained. 
As the backed up data grows, the index overflows the amount of RAM available and must be paged to disk. 
Without locality, the index cannot be cached effectively, and it is common for nearly 
every index access to require a random disk access. This disk bottleneck severely limits deduplication throughput.

This problem has been addressed by many previous studies. Zhu\cite{bottleneck08} tackle it 
by using an in-memory Bloom Filter and prefetch groups of chunk IDs that are likely to be 
accessed together with high probability. Lillibridge\cite{sparseindex09} break list of chunks 
into large segments, the chunk IDs in each incoming segment are sampled and the segment is 
deduplicated by comparing with the chunk IDs of only a few carefully selected backed up segments. 
These are segments that share many chunk IDs with the incoming segment with high probability.
Deepavali\cite{extreme_binning09} uses Broder's theorem\cite{resemblance97} to find similar files and group them
into the same physical location (bins) to deduplicate against each other.

In cloud storage, we are solving data deduplication problem in a different context of 
data stream deduplication (the D2D case). 
We now consider the case for file synchronization service designed to service fine-grained low-locality backup workloads. 
Such a workload consists of newly modified files, instead of large data streams, that arrive in random order from many clients. 
There is generally no locality between files that arrive in a given window of time. 
In the absence of locality, deduplication approach designed for D2D backups perform poorly.

In addition, file storage with deduplication allow a data chunk to be shared by many files,
this has not been a problem in D2D backup systems because they may use costly hardware components
and RAID to reduce the risk of data loss. Unlike D2D backup systems, cloud storage prefers 
low-cost hardware at large scale to achieve both performance and reliability. Without software
replication, data is fragile in the constant presense of disk or system failure.

Our solution, Sloud, is designed for data deduplication in cloud storage systems. 
Sloud exploits file similarity and only in-file data locality.
Like Sparse Indexing, it splits large files into small chunks and coarse-grain segments.
Sloud uses segment as the basic unit for storage allocation, thus avoiding the possibility that a set of 
similar large files may overload part of the system. 
Then a stastic based signature algorithm is used to cluster similar segments into containers
which are located at fixed places by stateless routing. When new segment comes in, 
Sloud finds the container which is most likely to have similar data, then it store new segment
to that container for chunk deduplication. By doing this Sloud trade deduplication space efficiency
for performance and simplicity. In addition, Sloud provides software replication and eventual consistency
to enhance file availability and integrity.
Experiments shows our solution achieves a balance of space efficiency, data reliability and load balancing.
